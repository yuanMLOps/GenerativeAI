{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64ff22f8-d784-4fd0-8d6a-a6c4807968fb",
   "metadata": {},
   "source": [
    "# Fine Tune for LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0c6d878-f6da-4b4c-9aa9-de070bbc9c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --disable-pip-version-check \\\n",
    "    torch==1.13.1 \\\n",
    "    torchdata==0.5.1 --quiet\n",
    "\n",
    "%pip install \\\n",
    "    transformers==4.27.2 \\\n",
    "    datasets==2.16.0 \\\n",
    "    evaluate==0.4.0 \\\n",
    "    rouge_score==0.1.2 \\\n",
    "    loralib==0.1.1 \\\n",
    "    peft==0.3.0 --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71e8a27-158c-49b9-a131-9dadbc60f075",
   "metadata": {},
   "source": [
    "## FLAN-T5 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "031fbf7b-4887-44cb-b6f8-5a13b43c1db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\n",
    "import torch\n",
    "import time\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf8cc31-c99b-4698-837e-61d41564a2d4",
   "metadata": {},
   "source": [
    "### Evaluate the Original Model\n",
    "#### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b8ddde4-2e2a-4dc1-ae36-11a0eb75ab68",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='google/flan-t5-base'\n",
    "\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecabb68d-87d3-4f4b-be2c-5bce087ab0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 247577856\n",
      "all model parameters: 247577856\n",
      "percentage of trainable model parameters: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# define a function to count the trainable and all parameters in a pytorch model\n",
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable = sum([param.numel() for _, param in model.named_parameters() if param.requires_grad])\n",
    "    all = sum([param.numel() for _, param in model.named_parameters()])\n",
    "    return f\"trainable model parameters: {trainable}\\nall model parameters: {all}\\npercentage of trainable model parameters: {100 * trainable / all:.2f}%\"\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(original_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08305a25-3a09-4cdb-a493-1538079fb0fa",
   "metadata": {},
   "source": [
    "#### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f50b9e4f-49c9-4470-bbbf-9401d4e5be27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 12460\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset('knkarthick/dialogsum')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ed90e4-da37-4922-a4d4-9f79372076be",
   "metadata": {},
   "source": [
    "#### Test the Model with Zero Shot Inferencing\n",
    "\n",
    "Test the model with the zero shot inferencing. The model extracted some information from the dialogue, but the summary obtained was not complete and comparable to the baseline human base summary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9cb41bac-29bb-42f9-aa1e-95b110f84d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Summarize the following conversation.\n",
      "\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "\n",
      "Summary:\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "#Person1#: I'm thinking of upgrading my computer.\n"
     ]
    }
   ],
   "source": [
    "# select a dialogue with index = 200\n",
    "index = 200\n",
    "\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "summary = dataset['test'][index]['summary']\n",
    "\n",
    "# construct the prompt\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "# tokenize and input the prompt and output the summary\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    original_model.generate(\n",
    "        inputs[\"input_ids\"], \n",
    "        max_new_tokens=200,\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "# format the output\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{prompt}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1635f644-8d6b-4e42-98db-7146cced8533",
   "metadata": {},
   "source": [
    "### Full Fine Tune of Flan-T5 Model by Human Labelled Summaries\n",
    "#### Preparation for Full Fine Tuning\n",
    "  * construct the prompts including instruction as the start prompt, dialogue and end prompt\n",
    "  * tokenize the prompts as 'input_ids' column and the labelled summary as 'labels' column in the dataset\n",
    "  * The processing is defined and executed in `tokenize_function` that will process each example in the training, validation and test datasets\n",
    "  * after processing all examples, filter out id, topic, dialogue and summary columns. Only keep `input_ids` and labels columns for fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92cb8fce-bdb7-49fe-a244-22d3fdb3e2f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ee912b46c8a485f9e4e5040f97770be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(example):\n",
    "    start_prompt = 'Summarize the following conversation.\\n\\n'\n",
    "    end_prompt = '\\n\\nSummary: '\n",
    "    prompt = [start_prompt + dialogue + end_prompt for dialogue in example[\"dialogue\"]]\n",
    "    example['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    example['labels'] = tokenizer(example[\"summary\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    \n",
    "    return example\n",
    "\n",
    "# The tokenize_function traverse the training, validation and test datasets in batches.\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['id', 'topic', 'dialogue', 'summary',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3995eac7-8a5e-4e24-8f0c-447dc7894d1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da37035ec054486ea19a75e21bffa915",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of the datasets:\n",
      "Training: (125, 2)\n",
      "Validation: (5, 2)\n",
      "Test: (15, 2)\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 125\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 5\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 15\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# only select a subset of the datasets for training and validation and test to reduce the time and resources for training\n",
    "tokenized_datasets = tokenized_datasets.filter(lambda example, index: index % 100 == 0, with_indices=True)\n",
    "print(f\"Shapes of the datasets:\")\n",
    "print(f\"Training: {tokenized_datasets['train'].shape}\")\n",
    "print(f\"Validation: {tokenized_datasets['validation'].shape}\")\n",
    "print(f\"Test: {tokenized_datasets['test'].shape}\")\n",
    "\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d11c7a-2c57-4b20-b3f2-3c26912ae9c8",
   "metadata": {},
   "source": [
    "##### Set Up TrainingArguments and Trainer to Fine Tune the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eff39eaa-2dfa-4b6e-b088-f83130c8770d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_dir = f'./dialogue-summary-training-{str(int(time.time()))}'\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=1,\n",
    "    max_steps=1\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=original_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation']\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c9abe8-6ce6-43a7-a20c-a558e90dfd14",
   "metadata": {},
   "source": [
    "#### Evaluate the Model Qualitatively (Human Evaluation)\n",
    "\n",
    "To evaluate the fine tuned model, select a dialogue, its predicted summary and the corresponding human baseline summary and compare the results. Compared to original_model, the summary of the fully trained instruct_model is much closer to the human baseline summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bfdb37-2b02-4051-a279-849bbe1cf1ef",
   "metadata": {},
   "source": [
    "* For comparison, first download the fully fine tuned T5 model using the entire traing dataset from s3 \n",
    "```shell\n",
    "aws s3 cp --recursive s3://dlai-generative-ai/models/flan-dialogue-summary-checkpoint/ ./flan-dialogue-summary-checkpoint/\n",
    "```\n",
    "* Now load the load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea77f135-7cca-45f2-ada5-8c3a51ef88aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct_model = AutoModelForSeq2SeqLM.from_pretrained(\"./flan-dialogue-summary-checkpoint\", torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5340a0-0a18-454a-a96e-9f3a86010193",
   "metadata": {},
   "source": [
    "* Select the dialogue with index of 200 to review the generated summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35ef406b-7213-4985-b04d-0406023aabe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "ORIGINAL MODEL:\n",
      "#Person1#: I'm thinking of upgrading my computer.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INSTRUCT MODEL:\n",
      "#Person1# suggests #Person2# upgrading #Person2#'s system, hardware, and CD-ROM drive. #Person2# thinks it's great.\n"
     ]
    }
   ],
   "source": [
    "index = 200\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "human_baseline_summary = dataset['test'][index]['summary']\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{human_baseline_summary}')\n",
    "print(dash_line)\n",
    "print(f'ORIGINAL MODEL:\\n{original_model_text_output}')\n",
    "print(dash_line)\n",
    "print(f'INSTRUCT MODEL:\\n{instruct_model_text_output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb09752-9c41-41a7-b9bf-c3617dfd1b71",
   "metadata": {},
   "source": [
    "#### Evaluate the Model Quantitatively (with ROUGE Metric)\n",
    "\n",
    "We can evaluate the generated summaries with the human baseline summaries by the [ROUGE metric](https://en.wikipedia.org/wiki/ROUGE_(metric)) by comparing the words from the two summriess. Results showed that an increase in the ROOUGE metric in summaries after fine-tuning.\n",
    "\n",
    "In the next cell, we define a ModelEval class to compare two models by calcuating the ROGUE metrics of the model generated summaries on input dialogues based on the corresponding human generated baseline summaries as the reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "edc1af57-2a39-4fcf-8550-b4c621d8dd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "class ModelEval:\n",
    "    rouge = evaluate.load('rouge')\n",
    "    def __init__(self, model_1_name, model_1, model_2_name, model_2, tokenizer, dialogues, ref_summaries):\n",
    "\n",
    "        # make sure the dialogues and referenc summaries have the same lengths\n",
    "        assert(len(dialogues) == len(ref_summaries))        \n",
    "\n",
    "        # define the prompt template to embed the dialogue and generate prompt to the model\n",
    "        self.prompt_temp = \"\"\"\n",
    "            Summarize the following conversation.\n",
    "        \n",
    "            {diag}\n",
    "        \n",
    "            Summary: \"\"\"\n",
    "        \n",
    "        self.model_1_name, self.model_1 = model_1_name, model_1\n",
    "        self.model_2_name, self.model_2 = model_2_name, model_2\n",
    "        self.tokenizer = tokenizer\n",
    "        self.ref_summaries = ref_summaries\n",
    "        \n",
    "        # generate prompts from dialogues, and the model generated summaries from each prompts for the two models in ocmparison\n",
    "        self.dialogues = [self.prompt_temp.format(diag=diag) for diag in dialogues]\n",
    "        self.model_1_summaries = [self.__class__.get_prediction(self.model_1, self.tokenizer, diag) for diag in self.dialogues]\n",
    "        self.model_2_summaries = [self.__class__.get_prediction(self.model_2, self.tokenizer, diag) for diag in self.dialogues]      \n",
    "       \n",
    "\n",
    "    @classmethod\n",
    "    def get_prediction(cls, model, tokenizer, dialogue) -> List[str]:\n",
    "        input_ids = tokenizer(dialogue, return_tensors=\"pt\").input_ids\n",
    "        output_ids = model.generate(input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))[0]\n",
    "        return tokenizer.decode(output_ids, skip_special_tokens=True)     \n",
    "\n",
    "    @classmethod\n",
    "    def eval_rogue(cls, pred_summaries, ref_summaries) -> float:\n",
    "        return cls.rouge.compute(\n",
    "            predictions=pred_summaries,\n",
    "            references=ref_summaries,\n",
    "            use_aggregator=True,\n",
    "            use_stemmer=True,\n",
    "        )\n",
    "        \n",
    "    def compare_models(self) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        calculate the ROGUE metrics for the two models in comparison and print out improvements in ROGUE metrics        \n",
    "        \"\"\"\n",
    "        # calculate the ROGUE scores for the two models\n",
    "        model_1_score = self.__class__.eval_rogue(self.model_1_summaries, self.ref_summaries)\n",
    "        model_2_score = self.__class__.eval_rogue(self.model_2_summaries, self.ref_summaries)\n",
    "\n",
    "        # calculate the improvement in ROGUE metrics and print out improvments\n",
    "        improvement = (np.array(list(model_2_score.values())) - np.array(list(model_1_score.values())))\n",
    "        print(f\"the improvement of {self.model_2_name} compared to {self.model_1_name} are the following\")\n",
    "        for key, value in zip(model_1_score.keys(), improvement):\n",
    "            print(f'{key}: {value*100:.2f}%')\n",
    "        return {self.model_1_name: model_1_score, self.model_2_name: model_2_score}\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c8be4479-36ea-4b60-a25d-56aefeecfd48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the improvement of instruct_model compared to original_model are the following\n",
      "rouge1: 21.11%\n",
      "rouge2: 15.91%\n",
      "rougeL: 10.00%\n",
      "rougeLsum: 10.00%\n",
      "{'original_model': {'rouge1': 0.16666666666666666, 'rouge2': 0.0, 'rougeL': 0.16666666666666666, 'rougeLsum': 0.16666666666666666}, 'instruct_model': {'rouge1': 0.37777777777777777, 'rouge2': 0.1590909090909091, 'rougeL': 0.26666666666666666, 'rougeLsum': 0.26666666666666666}}\n"
     ]
    }
   ],
   "source": [
    "# compare the two models\n",
    "dialogues = dataset['test'][0:3]['dialogue']\n",
    "ref_summaries = dataset['test'][0:3]['summary']\n",
    "model_1_name, model_1 = \"original_model\", original_model\n",
    "model_2_name, model_2 = \"instruct_model\", instruct_model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "meval = ModelEval(model_1_name, model_1, model_2_name, model_2, tokenizer, [dialogues[0]], [ref_summaries[0]])\n",
    "print(meval.compare_models())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e050f70-41e7-4fc0-8a9d-4041015c8297",
   "metadata": {},
   "source": [
    "### Perform Parameter Efficient Fine-Tuning (PEFT)\n",
    "\n",
    "**Parameter Efficient Fine-Tuning (PEFT)** fine-tuning is a form of instruction fine-tuning can usually obtain comparable results to the full fine-tuning, but with much less paramters to tune, which is much more efficient than full fine-tuning.\n",
    "\n",
    "Here, we use **Low-Rank Adaptation (LoRA)** as a PEFT technology to fine tuen the FLAN-T5 model. LoRA is usually used to fine-tune a model for a specific task. Note that LoRA doesn't change the original LLM model. When serving the inference requests, we combine the original LLM model with the newly-trained “LoRA adapter” as shown in the example. The LoRA adapter is only a several % of the original LLM size (MBs vs GBs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194d4948-3cf2-4086-8afe-86955f4b7dbc",
   "metadata": {},
   "source": [
    "#### Setup the PEFT/LoRA model for Fine-Tuning\n",
    "* define the LoraConfig for Flan-T5. For config paramters of commonly used llms, refer to [this link for reference](https://github.com/sematic-ai/sematic/blob/main/sematic/examples/summarization_finetune/__main__.py)\n",
    "* Note the rank (`r`) defines the rank/dimension of the adapter to be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0db353b9-ee44-4856-af2d-044e1cd76bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=32, # Rank\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM # FLAN-T5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "45642817-5701-4e1e-a6cf-fffce6057a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 3538944\n",
      "all model parameters: 251116800\n",
      "percentage of trainable model parameters: 1.41%\n"
     ]
    }
   ],
   "source": [
    "peft_model = get_peft_model(original_model, lora_config)\n",
    "print(print_number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d73fb14-7716-4b97-8325-49a52aff246d",
   "metadata": {},
   "source": [
    "#### Train Model Using PEFT/LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "565eb43a-e9ff-4aba-aa92-293742e09e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f'./peft-dialogue-summary-training-{str(int(time.time()))}'\n",
    "\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    auto_find_batch_size=True,\n",
    "    learning_rate=1e-3, # Higher learning rate than full fine-tuning.\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=1,\n",
    "    max_steps=1    \n",
    ")\n",
    "    \n",
    "peft_trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=peft_training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    ")\n",
    "\n",
    "peft_trainer.train()\n",
    "\n",
    "peft_model_path=\"./peft-dialogue-summary-checkpoint-local\"\n",
    "\n",
    "peft_trainer.model.save_pretrained(peft_model_path)\n",
    "tokenizer.save_pretrained(peft_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd66f719-5df7-4d84-a07f-c01b54bf448d",
   "metadata": {},
   "source": [
    "#### Use PEFT (LoRA) Fine Tuned Model with Base Model for Inference\n",
    "* The key point is that the base model weights are frozen during LoRA fine tune\n",
    "* The code demonstrate how to combine base and PEFT/LoRA fine tuned models to summarize dialogues\n",
    "* basically, you load both models using `PeftModel.from_pretrained()` function, and define the data type\n",
    "* the `torch_dtype` is set at torch.bfloat16, and `is_trainable` is False, since we will use the model for prediction, not for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "693170bc-ff05-4089-9eb5-5679930be206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use peft and the base model to summarize dialogues\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "# define the base model to fine tune\n",
    "peft_model_base = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\", torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(peft_model_base, \n",
    "                                       './peft-dialogue-summary-checkpoint-from-s3/', \n",
    "                                       torch_dtype=torch.bfloat16,\n",
    "                                       is_trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3a308584-df33-41ba-8f18-c4ccfe41bba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 0\n",
      "all model parameters: 251116800\n",
      "percentage of trainable model parameters: 0.00%\n"
     ]
    }
   ],
   "source": [
    "print(print_number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edef5473-0a1f-4fa3-915d-e185983f1159",
   "metadata": {},
   "source": [
    "#### Evaluate the Model Qualitatively (Human Evaluation)\n",
    "* the PEFT/LoRA fine tuned model generated more relavant summary compared to original model\n",
    "* Now lets evaluate the model by ROGUE metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a487bb69-ff3f-4dfb-91de-44a7c18398c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "ORIGINAL MODEL:\n",
      "#Person1#: I'm thinking of upgrading my computer.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INSTRUCT MODEL:\n",
      "#Person1# suggests #Person2# upgrading #Person2#'s system, hardware, and CD-ROM drive. #Person2# thinks it's great.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "PEFT MODEL: #Person1# recommends adding a painting program to #Person2#'s software and upgrading hardware. #Person2# also wants to upgrade the hardware because it's outdated now.\n"
     ]
    }
   ],
   "source": [
    "index = 200\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "baseline_human_summary = dataset['test'][index]['summary']\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary: \"\"\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "peft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{human_baseline_summary}')\n",
    "print(dash_line)\n",
    "print(f'ORIGINAL MODEL:\\n{original_model_text_output}')\n",
    "print(dash_line)\n",
    "print(f'INSTRUCT MODEL:\\n{instruct_model_text_output}')\n",
    "print(dash_line)\n",
    "print(f'PEFT MODEL: {peft_model_text_output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbea468a-5cac-43c7-92da-8fac55232dba",
   "metadata": {},
   "source": [
    "### Evaluate the Model Quantitatively (with ROUGE Metric)\n",
    "* Perform inferences for the sample of the test dataset (only 10 dialogues and summaries to save time)\n",
    "* Here we use our ModeEval class object for model evaluation, as shown in Full Fine Tune section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bfd0785a-2ceb-4107-a99d-5fdb6fd379ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#Person1# asks Ms. Dawson to take a dictation to all employees by this afternoon. Ms. Dawson tells #Person1# that all office communications are restricted to email correspondence and official memos. #Person1# wants to change the communication methods and asks Ms. Dawson to get the memo typed up and distributed to all employees before 4 pm.'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_prediction_peft(dialogue, tokenizer):\n",
    "    prompt = f\"\"\"\n",
    "        Summarize the following conversation.\n",
    "\n",
    "        {dialogue}\n",
    "\n",
    "        Summary: \"\"\"\n",
    "\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "    peft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "    peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return peft_model_text_output    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8a469293-a525-4f1a-98af-88a877b8b7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogues = dataset['test'][0:5]['dialogue']\n",
    "ref_summaries = dataset['test'][0:5]['summary']\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "peft_pred_summaries = [get_prediction_peft(dia, tokenizer) for dia in dialogues]\n",
    "origin_pred_summaries = [ModelEval.get_prediction(original_model, tokenizer, dia) for dia in dialogues]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0b67bd7d-aa23-4673-b9c2-af648fa603ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "original_rogue = ModelEval.eval_rogue(origin_pred_summaries, ref_summaries)\n",
    "peft_rogue = ModelEval.eval_rogue(peft_pred_summaries, ref_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f19f7c7d-ad81-4f87-a6c9-0626bd22bc93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the improvement of peft_model compared to original_model are the following\n",
      "rouge1: 11.33%\n",
      "rouge2: 2.65%\n",
      "rougeL: 8.17%\n",
      "rougeLsum: 8.38%\n",
      "rogue for peft\n",
      "{'rouge1': 0.34193513803269904, 'rouge2': 0.1022864276796861, 'rougeL': 0.27097500453912726, 'rougeLsum': 0.2728511771470072}\n",
      "rogue for original\n",
      "{'rouge1': 0.22858627858627859, 'rouge2': 0.0758237689744539, 'rougeL': 0.18922567498726442, 'rougeLsum': 0.18905242481401419}\n"
     ]
    }
   ],
   "source": [
    "improvement = (np.array(list(peft_rogue.values())) - np.array(list(original_rogue.values())))\n",
    "print(f\"the improvement of peft_model compared to original_model are the following\")\n",
    "for key, value in zip(peft_rogue.keys(), improvement):\n",
    "    print(f'{key}: {value*100:.2f}%')\n",
    "print(\"rogue for peft\")\n",
    "print(peft_rogue)\n",
    "print(\"rogue for original\")\n",
    "print(original_rogue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1266468-b27a-4a10-bd33-ffaaaf61d36c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
