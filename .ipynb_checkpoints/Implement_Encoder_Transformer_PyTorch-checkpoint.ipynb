{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77575f41-6245-4ac7-b6cf-ad5662644638",
   "metadata": {},
   "source": [
    "### Build the Econder part of a Transformer from the scratch based on the paper\n",
    "* This notebook implemented a transformer consists of multiple Encoder layers from the scratch\n",
    "* Each layer and its functionalities was implemented by Pytorch following the paper [attention is all you need](https://arxiv.org/pdf/1706.03762.pdf)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "591b6c02-4937-4ac5-b66c-f5423e938d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ccea2dcc-e47b-4984-8008-476931403c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, d_in: int, d_out: int, block_size: int, dropout:int, num_heads:int):\n",
    "        \"\"\"\n",
    "        initialize MultiHeadAttend Layer\n",
    "        \n",
    "        args:\n",
    "            d_in (int): dimension of input\n",
    "            d_out (int): dimension of output\n",
    "            block_size (int): maximum sequent length\n",
    "            dropout (int): percentage of drop out for dropout layers\n",
    "            num_heads (int): number of heads in the MultiHeadAttention layer\n",
    "        returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by n_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # dimension for each head\n",
    "\n",
    "        # define paramter matrices for query, key and value\n",
    "        self.W_query = torch.nn.Linear(d_in, d_out, bias=False)\n",
    "        self.W_key = torch.nn.Linear(d_in, d_out, bias=False)\n",
    "        self.W_value = torch.nn.Linear(d_in, d_out, bias=False)\n",
    "\n",
    "        # the optional linear layer as the last output layer\n",
    "        self.out_proj = torch.nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        \n",
    "        # dropout layer to reduce overfitting and speed up computations\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        \n",
    "        # define the upper triaglar matrix to mask the words after the current word to prevent leakage\n",
    "        # here we use block_size to generate the mask matrix with the largest possible dimensions\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(block_size, block_size), diagonal=1))\n",
    "\n",
    "    def split_heads(self, x:torch.Tensor):\n",
    "        \"\"\"\n",
    "        Splits the input tensor's features (d_out) to num_heads sections. Each head has self.head_dim features\n",
    "        args:\n",
    "            x: input tensor of the shape (batch_size, seq_length, d_out)\n",
    "        returns:\n",
    "            a tensor of the shape (batch_size, seq_length, num_heads, head_dim)\n",
    "        \"\"\"\n",
    "        # Reshape the input to have num_heads for multi-head attention\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        \"\"\"\n",
    "        Combines the featurs from multiple heads back to the original shape\n",
    "        args:\n",
    "            x: tensor of the shape (batch_size, seq_length, num_heads, head_dim)\n",
    "        returns:\n",
    "            a tensor with the shape (batch_size, seq_length, d_out) where num_heads * head_dim = d_out\n",
    "        \"\"\"\n",
    "        # Combine the multiple heads back to original shape\n",
    "        batch_size, num_heads, seq_length, head_dim = x.size()\n",
    "\n",
    "        # move the num_heads and head_dim as the last two dimensions, and then combine them \n",
    "        # back to d_out dimension\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_out)\n",
    "    \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):  \n",
    "        \"\"\"\n",
    "        caculate attention scores as the dot product of query and key vectors for each query\n",
    "        and then normalize the attention score by softmax and dropout layer\n",
    "        \"\"\"\n",
    "        # Calculate attention scores. This results in a seq_length * seq_length matrix\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        # Apply mask if provided. We use self.mask, which is a upper triangle matrix\n",
    "        # the upper elements are 1s, and will be converted to -tarch.inf. In the following\n",
    "        # softmax transformation, the attention scores of these positions will be zeros. Thus\n",
    "        # We hide words after the current word to prevent information leakage for encoder\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill_(mask, -torch.inf)\n",
    "        \n",
    "        # Softmax is applied to obtain attention probabilities\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        # apply dropout to the attn_probs\n",
    "        attn_probs = self.dropout(attn_probs)\n",
    "        \n",
    "        # Multiply by values to obtain the final output        \n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output \n",
    "        \n",
    "    def forward(self, Q, K, V, if_mask: bool):\n",
    "        seq_length = Q.size()[1]\n",
    "        \n",
    "        # Apply linear transformations and split heads\n",
    "        Q = self.split_heads(self.W_query(Q))\n",
    "        K = self.split_heads(self.W_key(K))\n",
    "        V = self.split_heads(self.W_value(V))\n",
    "        mask = None\n",
    "        if if_mask:\n",
    "            mask = self.mask.bool()[:seq_length, :seq_length].unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        # Perform scaled dot-product attention\n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # Combine heads and apply output transformation\n",
    "        output = self.out_proj(self.combine_heads(attn_output))\n",
    "        return output\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5973bd10-0264-4a3c-9d95-614539929085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement PositionWiseFeedForward as two linear layer followed by ReLU activation\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_out, d_ff):\n",
    "        \"\"\"\n",
    "        define and initialize PositionWiseFeedforward by a linear transformation, a\n",
    "        relu activation and a final linear transformation back to d_out dimension\n",
    "        \"\"\"\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_out, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_out)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4384508e-fdf1-42cf-8ccc-10172b7583e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement PositionalEncoding using Sin and Cos functions\n",
    "# an easier implementation as mentioned in the paper is to\n",
    "# use embedding layer to encode positions to vectors having d_out dimension\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_out, max_seq_length):\n",
    "        \"\"\"\n",
    "        initialize PositionalEncoding matrix and fill the matrix using\n",
    "        sin and cos for even and odd columns, respectively\n",
    "        args:\n",
    "            d_out (int): the output dimension, or the \"total\" model dimension\n",
    "            max_seq_length: the max length of the input sequence the model can handle\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        # construct the positional encoding matrix\n",
    "        pe = torch.zeros(max_seq_length, d_out)\n",
    "\n",
    "        # establish the n * 1 tensor where n = max_seq_length using unsqueeze(1)\n",
    "        # this provides the row for Positional Encoding matrix\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        # calculate the columns for Encoding matrix before sin/cos transformation        \n",
    "        div_term = torch.exp(torch.arange(0, d_out, 2).float() * -(math.log(10000.0) / d_out))\n",
    "        \n",
    "        # assign values for pe positional encoding matrix using sin and cos transformations\n",
    "        # for even and odd columns (refer to the publicated paper for how transformations were defined)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # add batch size dimension and store the pe matrix since we don't need to train these parameters\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        add input x vector, which is learned from attention mechanism \n",
    "        to the positional encoded vactor \n",
    "        note that self.pe has a dimension of (batch_size, seq_length, d_out)\n",
    "        this is the same as the matrix from embedding of input tokens and therefore\n",
    "        these two matrix can be added\n",
    "        \"\"\"\n",
    "        \n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "49a7b6b0-0031-4d0c-a359-2f35bd85bc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement EncoderLayer as described by the paper\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_in, d_out, max_seq_len, num_heads, d_ff, dropout):\n",
    "        \"\"\"\n",
    "        combines the self attention computed by MultiHeadAttention layer with original input tensor (as residual connection),\n",
    "        apply layer normalization. This tensor and the resulting tensor of positionwiseFeedForward of this tensor were added, \n",
    "        and finally apply layer normalization. This just follows the paper\n",
    "        args:\n",
    "            d_in: input dimension\n",
    "            d_out: output dimension, or model's feature dimension\n",
    "            max_seq_len: the maximum length of the sequences the model can handle\n",
    "            num_heads: the number of heads\n",
    "            d_ff: the internal dimension used in PositionWiseFeedForward linear transformation\n",
    "            dropout: dropout probability used in dropout layers\n",
    "        \"\"\"\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        self.self_attn = MultiHeadAttention(d_in, d_out, max_seq_len, dropout, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_out, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_out)\n",
    "        self.norm2 = nn.LayerNorm(d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, if_mask):\n",
    "        \"\"\"\n",
    "        apply self attention, residual connection, layer normalization, positionwiseFeedforward \n",
    "        and another residual connection and layer normalization to generate the output\n",
    "        \"\"\"\n",
    "        attn_output = self.self_attn(x, x, x, if_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "446c1d2f-1942-406d-89d9-b8bae7056cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this class implemented the encoder transformer by integrating encoder embedding, position encoding layers with\n",
    "# multiple encoder layers together, and finally, output the raw results as tensors in shape (batch_size, seq_length, vocab_size)\n",
    "class Encode_Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_in, d_out, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        \"\"\"\n",
    "        Initialize and set the embedding and positional_econding layers, the ecoder_layers and a linear layer to convert d_out dimensions\n",
    "        to vocab_size dimension, resulting in output tensor in (batch_size, seq_length, vocab_size). This output tensor can be\n",
    "        used by CrossEntropyLoss to calculate the loss for optimization\n",
    "        \"\"\"\n",
    "        super(Encode_Transformer, self).__init__()\n",
    "\n",
    "        # define embedding and positional_encoding layers\n",
    "        self.encoder_embedding = nn.Embedding(vocab_size, d_out)\n",
    "        self.positional_encoding = PositionalEncoding(d_out, max_seq_length)\n",
    "\n",
    "        # define the repeated blocks of EncoderLayers\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_in, d_out, max_seq_length, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        \n",
    "        # define the linear layer to transform from d_out to vocab_size\n",
    "        self.fc = nn.Linear(d_out, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        \n",
    "        # combine the emcoder embedding and positional_encoding of input tokens\n",
    "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
    "        \n",
    "        # transform the enc_ouput throught the blocks of repeated encoder layers\n",
    "        enc_output = src_embedded\n",
    "        layer_index = 0\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            if layer_index == 0:\n",
    "                enc_output = enc_layer(enc_output, True)\n",
    "            else:\n",
    "                enc_output = enc_layer(enc_output, False)\n",
    "            layer_index += 1    \n",
    "            \n",
    "        # transform output with the right dimensions\n",
    "        output = self.fc(enc_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefeff8c-a4a5-4ab4-91ee-40014600accf",
   "metadata": {},
   "source": [
    "### Test Transformer\n",
    "#### Define tokenizer, dataset and dataloader to load data (The implementation of GPTDatasetcode is from [the link](https://github.com/rasbt/LLMs-from-scratch/blob/3eb9358cbe013af00c37c1c321de0c4e83c689da/ch03/01_main-chapter-code/multihead-attention.ipynb))\n",
    "* This is a commonly used implementation of Pytorch Dataset and implemented the two required methods, `__len__` and `__getitem__`\n",
    "* The text content was separated into two chunks: `input_chunck` and `target_chunck`. Each chunck was tokenized as token ids and returned when Dataloader encapsulating this dataset is iterated\n",
    "* Every `target_chunck` was one position right shifted relative to its corresponding `input_chunck`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c7bc07ab-c7f8-48aa-8be3-7e634b5088e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class GPTDataset(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt)\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "# This function was implemented to load datat to GPU device\n",
    "# def collate_fn(batch):\n",
    "#     data = [item[0].to(device) for item in batch]\n",
    "#     target = [item[1].to(device) for item in batch]\n",
    "#     return data, target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b5e58f-9ef0-418a-80cb-25a4685423ec",
   "metadata": {},
   "source": [
    "#### Read file, encode the content by gpt2 tokenizer, and load input and target tokens\n",
    "* here we used the gpt2 tokenizer from tiktoken package. The advantage of this tokenizer is that if there is a word not contained in vocabulary list, the tokenizer will further tokenize the word to sub-words or characters. Therefore, we don't need to consider the word out of vocabulary list.\n",
    "* we define the dataset using GPTDataset class, which returns the `input_chunck` and `target_chunck` as the input and labels for training\n",
    "* we then define a Dataloader using this dataset and a `batch_size`\n",
    "* gpt2 tokenizer has a vocabulary size of 50257, and I use 256 as the size of the embedding layer for encoder. In addition, I defined the number of heads as 2 for the attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f7935adc-e568-4de6-8c8b-efbf31d1dd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"the-verdict.txt\"\n",
    "def get_dataloader(file_path: str, encoding: str=\"utf-8\", batch_size:int =4, max_length:int=256, stride:int=128):\n",
    "    \"\"\"\n",
    "    given the file_path, generate tokens using gpt2 tokenizer, and return a DataLoader encapsulate GPTDataset\n",
    "    that tokenized the content of the file, with input_chunks and target_chunks. The target_chunks shift one position\n",
    "    to the input_chunks\n",
    "    \"\"\"\n",
    "        \n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=encoding) as f:\n",
    "            raw_text = f.read()\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: File not found.\")\n",
    "    except PermissionError:\n",
    "        print(\"Error: Permission denied.\")\n",
    "    except Exception as e:  # Catch any other unexpected errors\n",
    "        print(f\"An error occurred: {e}\")        \n",
    "    \n",
    "    # tokenize the content of the file and returns input and target chunks\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDataset(raw_text, tokenizer, max_length, stride)        \n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size) \n",
    "   \n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "37024ed3-154f-4270-9c42-c2f5fbaf8086",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 4\n",
    "\n",
    "model_config = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"max_seq_length\": 4,\n",
    "    \"d_in\": 256,\n",
    "    \"d_out\": 256,\n",
    "    \"num_heads\": 2,\n",
    "    \"num_layers\": 2,\n",
    "    \"d_ff\":2048,\n",
    "    \"dropout\": 0.1\n",
    "}\n",
    "\n",
    "et = Encode_Transformer(**model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "78d019b1-12a3-4d03-abdf-b9c9f05630f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 8.372800827026367\n",
      "Epoch: 2, Loss: 7.001795768737793\n",
      "Epoch: 3, Loss: 6.156527042388916\n",
      "Epoch: 4, Loss: 5.566163063049316\n",
      "Epoch: 5, Loss: 4.95515775680542\n",
      "Epoch: 6, Loss: 4.574779987335205\n",
      "Epoch: 7, Loss: 3.970862627029419\n",
      "Epoch: 8, Loss: 3.5937862396240234\n",
      "Epoch: 9, Loss: 3.2626678943634033\n",
      "Epoch: 10, Loss: 3.1146388053894043\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(et.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "text_file = \"the-verdict.txt\"\n",
    "dataloader = get_dataloader(text_file, batch_size=8, max_length=block_size, stride=5)\n",
    "\n",
    "et.train()\n",
    "\n",
    "for epoch in range(10):\n",
    "    for batch in dataloader:\n",
    "        x, y = batch        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # foramt output as in shape (batch_size * seq_len, vocab_size)\n",
    "        output = et(x).view(-1, vocab_size)\n",
    "        \n",
    "        # format y value as (batch_size * seq_len,) using view(-1) to\n",
    "        # get all the items as in one dimension\n",
    "        y = y.view(-1)\n",
    "        loss = criterion(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e9bc89-cd96-4533-824a-808df360de71",
   "metadata": {},
   "source": [
    "### Save models and reload model\n",
    "* follow the good practice to save the model's `state_dict` instead of the model itself, and rebuild the model from its class definition, and load the parameters from the saved `state_dict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5c2b684d-21dd-430e-8eb4-020556f1b99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "dir_path = Path('./models')\n",
    "dir_path.mkdir(parents=True, exist_ok=True)\n",
    "model_path = './models/encoder_transformer_v2.pth'\n",
    "torch.save(et.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0feb6fc8-5b8a-4c9a-ae59-8e8c53eb4092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocab_size = 50257\n",
    "# d_out = 256\n",
    "# d_in = d_out\n",
    "# block_size = 4\n",
    "# max_length = 4\n",
    "# num_heads = 2\n",
    "# num_layers = 2\n",
    "# d_ff = 2048\n",
    "# dropout = 0.1\n",
    "\n",
    "test_model_path = './models/encoder_transformer_test.pth'\n",
    "model = Encode_Transformer(**model_config)\n",
    "model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2dca168e-e4bd-476e-8013-78d93f0ee219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I HAD always', ' Jack Gisburn', ' a cheap genius--', ' a good fellow enough', 'so it was no', ' surprise to me to', ' that, in the', ' of his glory,']\n",
      "[' justAD always--', ' Gisburn had', '------and', ' little-- up--', '- was no--', ' to me to me', ', and the the', ' his,, so']\n"
     ]
    }
   ],
   "source": [
    "# load the first batch of text and test the results\n",
    "text_file = \"the-verdict.txt\"\n",
    "dl = get_dataloader(text_file, batch_size=8, max_length=block_size, stride=5)\n",
    "x,_ = next(iter(dl))\n",
    "with torch.no_grad():\n",
    "    rs = model(x)\n",
    "rs_ids = torch.argmax(torch.softmax(rs, dim=-1), dim=-1)\n",
    "print([tokenizer.decode(x[i].tolist()) for i in range(8)])\n",
    "print([tokenizer.decode(rs_ids[i].tolist()) for i in range(8)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086129e1-e14d-48f6-98e3-52deb0b44321",
   "metadata": {},
   "source": [
    "### Future work\n",
    "* separate the dataset into training, validation and test datasets for a \"real\" model training process\n",
    "* utilize Pytorch lightning to control and store the best model by defining the monitoring metrics and automatically  handle the usage of GPU depending on the availability of the computing resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ad7085-8e09-4f51-b14e-c57ded66f8c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
